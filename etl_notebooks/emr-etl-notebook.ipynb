{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85579387-5d01-4cd8-8b6f-bdb4ae914c0a",
   "metadata": {},
   "source": [
    "# EMR ETL Notebook\n",
    "- this notebook is tested to run on AWS EMR cluster with configuration listed in docs/aws_create_cluster.txt and config/spark-config\n",
    "- Contents\n",
    "    - **ETL Part I: preprocess raw data into parquet files**\n",
    "    - **ETL Part II: create dimensional model using the preprocessed data**\n",
    "  \n",
    "- outputs are tested in etl_notebooks/emr-etl-test-notebook.ipynb\n",
    "- /apps folder contains .py files with identical ETL code, they can be run using spark-submit\n",
    "- to run and test ETL Part III use redshift-etl-notebook.ipynb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067c6891-354f-4bb8-b31f-0c9813ef3414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79c853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Paths    \n",
    "TEST = False\n",
    "scrape_year_month = '2021-01'\n",
    "\n",
    "# S3\n",
    "path_global_listings = 'airbnb-listings.csv'\n",
    "path_city_listings = f'cities/*/{scrape_year_month}/listings.csv'    \n",
    "path_city_reviews = f'cities/*/{scrape_year_month}/reviews.csv'\n",
    "path_city_temperature = \"weather/ECA_blend_tg/*.txt\"\n",
    "path_city_rain = \"weather/ECA_blend_rr/*.txt\"\n",
    "\n",
    "raw_data_folder = \"raw\"\n",
    "# input_parquet_folder = \"input_parquets_notebook\"\n",
    "# dim_model_folder = \"dim_model_notebook\"\n",
    "# dim_model_folder_new = \"dim_model_notebook_temp\"\n",
    "input_parquet_folder = \"input_parquets_airflow\"\n",
    "dim_model_folder = \"dim_model_airflow\"\n",
    "dim_model_folder_new = \"dim_model_airflow_temp\"\n",
    "s3_path = \"s3://{}/{}/{}\"\n",
    "\n",
    "bucket_name = \"airbnbprj-us\"\n",
    "\n",
    "if TEST:\n",
    "    input_parquet_folder += \"_test\"\n",
    "    dim_model_folder += \"_test\"\n",
    "    dim_model_folder_new += \"_test\"\n",
    "\n",
    "raw_global_listings_path = s3_path.format(bucket_name, raw_data_folder, path_global_listings)\n",
    "raw_city_listings_path = s3_path.format(bucket_name, raw_data_folder, path_city_listings)\n",
    "raw_city_reviews_path = s3_path.format(bucket_name, raw_data_folder, path_city_reviews)\n",
    "raw_city_temperature_path = s3_path.format(bucket_name, raw_data_folder, path_city_temperature)\n",
    "raw_city_rain_data_path = s3_path.format(bucket_name, raw_data_folder, path_city_rain)\n",
    "\n",
    "path_out_global_listings = s3_path.format(bucket_name, input_parquet_folder, 'global_listings.parquet')\n",
    "path_out_city_listings_data = s3_path.format(bucket_name, input_parquet_folder, f'city_listings/{scrape_year_month}/city_listings.parquet')\n",
    "path_out_city_reviews_data = s3_path.format(bucket_name, input_parquet_folder, f'city_reviews/{scrape_year_month}/city_reviews.parquet')\n",
    "path_out_city_temperature_data = s3_path.format(bucket_name, input_parquet_folder, 'city_temperature.parquet')\n",
    "path_out_city_rain_data = s3_path.format(bucket_name, input_parquet_folder, 'city_rain.parquet')\n",
    "path_out_weather_stations = s3_path.format(bucket_name, input_parquet_folder, 'weather_stations.parquet')\n",
    "\n",
    "dim_model_listings = s3_path.format(bucket_name, dim_model_folder, 'listings.csv')\n",
    "dim_model_hosts = s3_path.format(bucket_name, dim_model_folder, 'hosts.csv')\n",
    "dim_model_reviews = s3_path.format(bucket_name, dim_model_folder, 'reviews.csv')\n",
    "dim_model_reviewers = s3_path.format(bucket_name, dim_model_folder, 'reviewers.csv')\n",
    "dim_model_weather = s3_path.format(bucket_name, dim_model_folder, 'weather.csv')\n",
    "\n",
    "dim_model_listings_new = s3_path.format(bucket_name, dim_model_folder_new, 'listings.csv')\n",
    "dim_model_hosts_new = s3_path.format(bucket_name, dim_model_folder_new, 'hosts.csv')\n",
    "dim_model_reviews_new = s3_path.format(bucket_name, dim_model_folder_new, 'reviews.csv')\n",
    "dim_model_reviewers_new = s3_path.format(bucket_name, dim_model_folder_new, 'reviewers.csv')\n",
    "dim_model_weather_new = s3_path.format(bucket_name, dim_model_folder_new, 'weather.csv')\n",
    "\n",
    "dim_model_reviews_step1 = s3_path.format(bucket_name, dim_model_folder_new, 'reviews_step1.csv')\n",
    "dim_model_reviews_step2 = s3_path.format(bucket_name, dim_model_folder_new, 'reviews_step2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4085cd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "raw_global_listings_path ,\n",
    "raw_city_listings_path ,\n",
    "raw_city_reviews_path ,\n",
    "raw_city_temperature_path ,\n",
    "raw_city_rain_data_path ,\n",
    "path_out_global_listings ,\n",
    "path_out_city_listings_data ,\n",
    "path_out_city_reviews_data ,\n",
    "path_out_city_temperature_data ,\n",
    "path_out_city_rain_data ,\n",
    "path_out_weather_stations ,\n",
    "dim_model_listings ,\n",
    "dim_model_hosts ,\n",
    "dim_model_reviews ,\n",
    "dim_model_reviewers ,\n",
    "dim_model_weather ,\n",
    "dim_model_listings_new ,\n",
    "dim_model_hosts_new ,\n",
    "dim_model_reviews_new ,\n",
    "dim_model_reviewers_new ,\n",
    "dim_model_weather_new,\n",
    "dim_model_reviews_step1,\n",
    "dim_model_reviews_step2]\n",
    "for path in paths:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248b623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e46a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_exists(path):\n",
    "    response = s3_client.list_objects(Bucket=bucket_name, MaxKeys=1, Prefix=path.replace(f\"s3://{bucket_name}/\",\"\"))\n",
    "    if 'Contents' not in response:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe77ef5",
   "metadata": {},
   "source": [
    "# Part I - preprocessing raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf499546",
   "metadata": {},
   "source": [
    "## Global listings\n",
    "- read as csv\n",
    "- drop columns that are not relevant\n",
    "- rename columns\n",
    "- save as parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8010f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(path_out_global_listings):\n",
    "    df_global_listings = spark.read.csv(raw_global_listings_path, header=\"True\", inferSchema=\"True\",multiLine=\"True\",escape='\"',ignoreLeadingWhiteSpace=\"True\", sep=\";\")\n",
    "    df_global_listings = df_global_listings.toDF(*[column.replace(\" \",\"_\").lower() for column in df_global_listings.columns])\n",
    "    columns_to_drop = ['xl_picture_url', 'cancellation_policy', 'access', 'features\\r', 'zipcode', 'country_code', 'smart_location',\\\n",
    "      'country', 'security_deposit', 'medium_url', 'transit', 'cleaning_fee', 'street', 'experiences_offered', \\\n",
    "      'thumbnail_url', 'extra_people', 'weekly_price', 'notes', 'house_rules', 'monthly_price', \\\n",
    "      'summary', 'square_feet', 'interaction', 'state','jurisdiction_names', 'market', 'geolocation', \\\n",
    "      'space', 'bed_type', 'guests_included']\n",
    "    df_global_listings = df_global_listings.drop(*columns_to_drop)\n",
    "    df_global_listings = df_global_listings.withColumn('scrape_year', F.year(F.col('last_scraped'))).withColumn('scrape_month', F.month(F.col('last_scraped')))\n",
    "    \n",
    "    if TEST:    \n",
    "        df_global_listings.filter(\"city = 'Amsterdam'\").write.partitionBy('scrape_year','scrape_month').parquet(path_out_global_listings)\n",
    "    else:    \n",
    "        df_global_listings.write.partitionBy('scrape_year','scrape_month').parquet(path_out_global_listings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae90584b",
   "metadata": {},
   "source": [
    "## Local listings (Amsterdam, Berlin, London, Paris), scraped in {scrape_year_month}\n",
    "- read as csv\n",
    "- extract new column city from filename\n",
    "- write as parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b2c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(path_out_city_listings_data):\n",
    "    df_city_listings = spark.read.csv(raw_city_listings_path, header=\"True\", inferSchema=\"True\",multiLine=\"True\",escape='\"',ignoreLeadingWhiteSpace=\"True\")\n",
    "    df_city_listings = df_city_listings.withColumn(\"city\",F.element_at(F.split(F.input_file_name(),\"/\"), -3))\n",
    "    df_city_listings = df_city_listings.withColumn('scrape_year', F.year(F.col('last_scraped'))).withColumn('scrape_month', F.month(F.col('last_scraped')))\n",
    "\n",
    "    if TEST:    \n",
    "        df_city_listings.filter(\"city = 'Amsterdam'\").write.partitionBy('scrape_year','scrape_month').parquet(path_out_city_listings_data)\n",
    "    else:    \n",
    "        df_city_listings.write.partitionBy('scrape_year','scrape_month').parquet(path_out_city_listings_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc46a82",
   "metadata": {},
   "source": [
    "## Local reviews (Amsterdam, Berlin, London, Paris) scraped in {scrape_year_month}\n",
    "- read as csv\n",
    "- extract new column city from filename\n",
    "- write as parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92e89b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(path_out_city_reviews_data):\n",
    "    df_city_reviews = spark.read.csv(raw_city_reviews_path, header=\"True\", inferSchema=\"True\",multiLine=\"True\",escape='\"',ignoreLeadingWhiteSpace=\"True\")\n",
    "    df_city_reviews = df_city_reviews.withColumn(\"city\",F.element_at(F.split(F.input_file_name(),\"/\"), -3))\n",
    "    df_city_reviews = df_city_reviews.withColumn('year', F.year(F.col('date'))).withColumn('month', F.month(F.col('date')))\n",
    "    \n",
    "    if TEST:    \n",
    "        df_city_reviews.filter(\"city = 'Amsterdam'\").write.partitionBy('year','month','city').parquet(path_out_city_reviews_data)\n",
    "    else:\n",
    "        df_city_reviews.write.partitionBy('year','month','city').parquet(path_out_city_reviews_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea554f0",
   "metadata": {},
   "source": [
    "## Weather data\n",
    "- read as text\n",
    "- skip multiline header and keep only rows with actual data\n",
    "- write as parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e06019",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(path_out_city_temperature_data):\n",
    "    text = sc.textFile(raw_city_temperature_path) \\\n",
    "        .map(lambda line: line.replace(\" \",\"\").split(\",\")) \\\n",
    "        .filter(lambda line: len(line)==5) \\\n",
    "        .filter(lambda line: line[0]!=\"STAID\")\n",
    "\n",
    "    df = spark.createDataFrame(text)  \n",
    "    columns = [\"STAID\", \"SOUID\", \"DATE\", \"TG\", \"Q_TG\"]\n",
    "    df = df.toDF(*columns)\n",
    "    df.write.parquet(path_out_city_temperature_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6ff35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(path_out_city_rain_data):\n",
    "    text = sc.textFile(raw_city_rain_data_path) \\\n",
    "        .map(lambda line: line.replace(\" \",\"\").split(\",\")) \\\n",
    "        .filter(lambda line: len(line)==5) \\\n",
    "        .filter(lambda line: line[0]!=\"STAID\")\n",
    "\n",
    "    df = spark.createDataFrame(text)  \n",
    "    columns = [\"STAID\", \"SOUID\", \"DATE\", \"RR\", \"Q_TG\"]\n",
    "    df = df.toDF(*columns)\n",
    "    df.write.parquet(path_out_city_rain_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d37882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create manual 'look-up' table joining weather station id's and city name\n",
    "if not model_exists(path_out_weather_stations):\n",
    "    station_city = [(593,'Amsterdam'), (41,'Berlin'), (1860,'London'),(11249,'Paris')]\n",
    "    columns = [\"STAID\",\"city\"]\n",
    "    df_stations = spark.createDataFrame(data=station_city, schema = columns)\n",
    "    df_stations.write.parquet(path_out_weather_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5c6b83",
   "metadata": {},
   "source": [
    "# Part II - Dimensional model\n",
    "The dimensional model comprises 5 tables:\n",
    "- Fact table\n",
    "    - reviews\n",
    "- Dimensional tables\n",
    "    - reviewers\n",
    "    - listings\n",
    "    - hosts\n",
    "    - weather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d645fb0",
   "metadata": {},
   "source": [
    "## Listings table\n",
    "- uniquely identified by listing_id\n",
    "- source data are extracted from raw listings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99563459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if listings table does not exit then read global listings data,\n",
    "# append new null columns and sort them alphabetically to allow merging with city listings data later    \n",
    "if not model_exists(dim_model_listings):\n",
    "    df_listings_global = spark.read.parquet(path_out_global_listings)\n",
    "    df_listings_global.createOrReplaceTempView(\"global\")\n",
    "    query=\"\"\"\n",
    "    SELECT *, cast(null as string) as bathrooms_text, cast(null as integer) as calculated_host_listings_count_entire_homes, cast(null as integer) as calculated_host_listings_count_private_rooms,\n",
    "     cast(null as integer) as calculated_host_listings_count_shared_rooms, cast(null as string) as host_has_profile_pic, cast(null as string) as host_identity_verified, cast(null as string) as host_is_superhost,\n",
    "     cast(null as string) as instant_bookable, cast(null as integer) as maximum_maximum_nights, cast(null as integer) as maximum_minimum_nights, cast(null as double) as maximum_nights_avg_ntm, cast(null as integer) as minimum_maximum_nights,\n",
    "     cast(null as integer) as minimum_minimum_nights, cast(null as double) as minimum_nights_avg_ntm, cast(null as integer) as number_of_reviews_l30d, cast(null as integer) as number_of_reviews_ltm\n",
    "    FROM global\n",
    "    \"\"\"\n",
    "    df_listings_hosts = spark.sql(query)\n",
    "    df_listings_hosts = df_listings_hosts.select(sorted(df_listings_hosts.columns))\n",
    "    df_listings_hosts = df_listings_hosts.withColumnRenamed(\"id\",\"listing_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52adf8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(dim_model_listings):\n",
    "    # drop columns that are later included in hosts table, keep host_id\n",
    "    columns_to_drop = [\"host_name\", \"host_url\", \"host_since\", \"host_location\", \"host_about\", \"host_response_time\", \"host_response_rate\", \"host_acceptance_rate\",\n",
    "    \"host_is_superhost\", \"host_thumbnail_url\", \"host_picture_url\", \"host_neighbourhood\", \"host_listings_count\",\n",
    "    \"host_total_listings_count\", \"host_verifications\", \"host_has_profile_pic\", \"host_identity_verified\"]\n",
    "    df_listings = df_listings_hosts.drop(*columns_to_drop)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ce7b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if listings table already exists then read it. It will be merged with monthly listings data later\n",
    "if model_exists(dim_model_listings):    \n",
    "    df_listings = spark.read.csv(dim_model_listings,header=\"True\", inferSchema=\"True\",multiLine=\"True\",escape='\"',ignoreLeadingWhiteSpace=\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c543475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read monthly listings data and sort columns to allow merging\n",
    "df_listings_hosts_monthly = spark.read.parquet(path_out_city_listings_data)\n",
    "df_listings_hosts_monthly = df_listings_hosts_monthly.select(sorted(df_listings_hosts_monthly.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db403a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that are later included in hosts table, keep host_id\n",
    "columns_to_drop = [\"host_name\", \"host_url\", \"host_since\", \"host_location\", \"host_about\", \"host_response_time\", \"host_response_rate\", \"host_acceptance_rate\",\n",
    "\"host_is_superhost\", \"host_thumbnail_url\", \"host_picture_url\", \"host_neighbourhood\", \"host_listings_count\",\n",
    "\"host_total_listings_count\", \"host_verifications\", \"host_has_profile_pic\", \"host_identity_verified\"]\n",
    "df_listings_monthly = df_listings_hosts_monthly.drop(*columns_to_drop).withColumnRenamed(\"id\",\"listing_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b79a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge global and local listings, drop duplicates by filtering by latest scrape date\n",
    "df_listings_updated = df_listings.union(df_listings_monthly)\n",
    "windowSpec  = Window.partitionBy(\"listing_id\").orderBy(\"last_scraped\").rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)                                             \n",
    "df_listings_updated = df_listings_updated.withColumn(\"latest\", F.last(\"last_scraped\").over(windowSpec))\\\n",
    "                      .filter(\"last_scraped == latest\")\\\n",
    "                      .dropDuplicates([\"listing_id\"])\\\n",
    "                      .drop('latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79246146",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings_updated.write.csv(dim_model_listings_new, escape='\"', header=\"true\")          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb8c30d",
   "metadata": {},
   "source": [
    "## Hosts table\n",
    "- uniquely identified by host_id\n",
    "- source data are extracted from raw listings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad06a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If dimensional model does not exist yet, then create hosts table from raw listings data\n",
    "if not model_exists(dim_model_listings):\n",
    "    df_listings_hosts.createOrReplaceTempView(\"listings\")\n",
    "    query=\"\"\"\n",
    "    SELECT host_id, host_name, host_url, host_since, host_location, host_about, host_response_time, host_response_rate, host_acceptance_rate,\n",
    "    host_is_superhost, host_thumbnail_url, host_picture_url, host_neighbourhood, host_listings_count,\n",
    "    host_total_listings_count, host_verifications, host_has_profile_pic, host_identity_verified, last_scraped     \n",
    "    FROM listings\n",
    "    \"\"\"\n",
    "    df_hosts = spark.sql(query)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01026d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(dim_model_listings):\n",
    "    # drop duplicates to keep unique hosts\n",
    "    windowSpec  = Window.partitionBy(\"host_id\").orderBy(\"last_scraped\").rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)                                             \n",
    "    df_hosts = df_hosts.withColumn(\"latest\", F.last(\"last_scraped\").over(windowSpec))\\\n",
    "                       .filter(\"last_scraped == latest\")\\\n",
    "                       .dropDuplicates([\"host_id\"])\\\n",
    "                       .drop('latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03d07da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if dimensional model already exists (testing listings model) then read the hosts table, it will be merged with hosts data from monthly listings data\n",
    "if model_exists(dim_model_listings):    \n",
    "    # read existing hosts table\n",
    "    df_hosts = spark.read.csv(dim_model_hosts,header=\"True\", inferSchema=\"True\",multiLine=\"True\",escape='\"',ignoreLeadingWhiteSpace=\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9f98a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create hosts table\n",
    "df_listings_hosts_monthly.createOrReplaceTempView(\"listings\")\n",
    "query=\"\"\"\n",
    "SELECT host_id, host_name, host_url, host_since, host_location, host_about, host_response_time, host_response_rate, host_acceptance_rate,\n",
    "host_is_superhost, host_thumbnail_url, host_picture_url, host_neighbourhood, host_listings_count,\n",
    "host_total_listings_count, host_verifications, host_has_profile_pic, host_identity_verified, last_scraped     \n",
    "FROM listings\n",
    "\"\"\"\n",
    "df_hosts_monthly = spark.sql(query)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1651e5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge hosts data extracted from montly listings data with existing host table\n",
    "df_hosts_updated = df_hosts.union(df_hosts_monthly)\n",
    "windowSpec  = Window.partitionBy(\"host_id\").orderBy(\"last_scraped\").rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)                                             \n",
    "df_hosts_updated = df_hosts_updated.withColumn(\"latest\", F.last(\"last_scraped\").over(windowSpec))\\\n",
    "                   .filter(\"last_scraped == latest\")\\\n",
    "                   .dropDuplicates([\"host_id\"])\\\n",
    "                   .drop('latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d80f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hosts_updated.write.csv(dim_model_hosts_new, escape='\"', header=\"true\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c1fe3c",
   "metadata": {},
   "source": [
    "## Reviews table\n",
    "- uniquely identified by reviews_id\n",
    "- contains \"foreign keys\" to other tables host_id, listing_id, weather_id, reviewer_id\n",
    "- source data are extracted from raw reviews data\n",
    "- includes new data obtained by NLP processing: comment_language, (comment) sentiment\n",
    "\n",
    "Data:\n",
    "- reviews table from existing dimensional model (if exists)\n",
    "- monthly reviews update containing some but not all reviews from previous months\n",
    "- listings table from dimensional model\n",
    "\n",
    "Steps:\n",
    "1. Find all rows in monthly update where review_id is not in dimensional model already (select all rows if there is no dimensional model)\n",
    "2. using 1: join with listings table from dimensional model to get host_id\n",
    "3. using 2: run language detection\n",
    "4. using 3: run sentiment analysis on english comments\n",
    "5. Combine rows (union) of existing dimensional model and result from step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca4c510",
   "metadata": {},
   "source": [
    "### Step 1&2: Find new reviews and update their host_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c7900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_monthly = spark.read.parquet(path_out_city_reviews_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1497d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings = spark.read.csv(dim_model_listings_new,header=\"True\", inferSchema=\"True\",multiLine=\"True\",escape='\"',ignoreLeadingWhiteSpace=\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978ecf2f-2f44-4557-bd0b-db0c1bdc4063",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(dim_model_reviews):\n",
    "    df_reviews_delta = df_reviews_monthly\n",
    "    \n",
    "else:\n",
    "    df_reviews = spark.read.csv(dim_model_reviews,header=\"True\", inferSchema=\"True\",multiLine=\"True\",escape='\"',ignoreLeadingWhiteSpace=\"True\")\n",
    "    df_reviews.createOrReplaceTempView(\"reviews\")\n",
    "    df_reviews_monthly.createOrReplaceTempView(\"reviews_monthly\")\n",
    "\n",
    "    query=\"\"\"\n",
    "    SELECT *\n",
    "    FROM reviews_monthly\n",
    "    WHERE reviews_monthly.date >= \n",
    "        (SELECT max(reviews.date)\n",
    "         FROM reviews)   \n",
    "    \"\"\"\n",
    "    df_reviews_delta = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_delta.createOrReplaceTempView(\"reviews_delta\")\n",
    "df_listings.createOrReplaceTempView(\"listings\")\n",
    "\n",
    "query=\"\"\"\n",
    "SELECT r.id as review_id, r.reviewer_id, r.listing_id, listings.host_id as host_id, concat_ws(\"_\",r.city, r.date) as weather_id, r.date, r.reviewer_name, r.comments \n",
    "FROM reviews_delta r\n",
    "LEFT JOIN listings\n",
    "ON r.listing_id == listings.listing_id\n",
    "\"\"\"\n",
    "df_reviews_delta = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70daca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_delta.write.csv(dim_model_reviews_step1, escape='\"', header=\"true\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32604277",
   "metadata": {},
   "source": [
    "### Step 3: Detect comment language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73c23e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    df_reviews_delta = df_reviews_delta.limit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8e5d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect language, translate, detect sentiment\n",
    "#spark = sparknlp.start()\n",
    "language_detector = PretrainedPipeline('detect_language_220', lang='xx')\n",
    "df_result = language_detector.annotate(df_reviews_delta, column=\"comments\")\n",
    "df_reviews_delta2 = df_result.withColumn(\"comment_language\", F.concat_ws(\",\",F.col(\"language.result\"))).drop(\"document\").drop(\"sentence\").drop(\"language\").withColumnRenamed('text','comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a7b590",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_delta2.write.csv(dim_model_reviews_step2, escape='\"', header=\"true\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99c0f89",
   "metadata": {},
   "source": [
    "### Step 4: Detect sentiment of english comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3ef627",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_delta2 = spark.read.csv(dim_model_reviews_step2,header=\"True\", inferSchema=\"True\",multiLine=\"True\",escape='\"',ignoreLeadingWhiteSpace=\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6e5cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer = PretrainedPipeline('analyze_sentimentdl_use_imdb', lang='en')\n",
    "df_result_sentiment = sentiment_analyzer.annotate(df_reviews_delta2.filter(F.col(\"comment_language\") == 'en'), column=\"comments\")\n",
    "df_result_sentiment = df_result_sentiment.withColumn(\"sentiment\", F.concat_ws(\",\",F.col(\"sentiment.result\"))).drop(\"document\").drop(\"sentence_embeddings\").withColumnRenamed('text','comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc8aca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_null = df_reviews_delta2.filter(\"comment_language is null\").withColumn(\"sentiment\", F.lit('n/a'))\n",
    "df_reviews_delta3 = df_reviews_delta2.filter(\"comment_language != 'en'\").withColumn(\"sentiment\", F.lit('n/a'))\\\n",
    "                    .union(df_result_sentiment)\\\n",
    "                    .union(df_reviews_null)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40adcd0",
   "metadata": {},
   "source": [
    "### Step 5: Combine to create new reviews table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f184b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_exists(dim_model_reviews):\n",
    "    df_reviews_delta3.write.csv(dim_model_reviews_new, escape='\"', header=\"true\")        \n",
    "else:\n",
    "    df_reviews = spark.read.csv(dim_model_reviews,header=\"True\", inferSchema=\"True\",multiLine=\"True\",escape='\"',ignoreLeadingWhiteSpace=\"True\")\n",
    "    df_reviews_updated = df_reviews.union(df_reviews_delta3)\n",
    "    # Its necessary to drop duplicates since some of the reviews submitted at the scrape date will be included twice\n",
    "    df_reviews_updated = df_reviews_updated.dropDuplicates([\"review_id\"])\n",
    "    df_reviews_updated.write.csv(dim_model_reviews_new, escape='\"', header=\"true\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881f88a6",
   "metadata": {},
   "source": [
    "## Reviewers table\n",
    "- uniquely identified by reviewer_id\n",
    "- it is extracted from the reviews table\n",
    "- includes new data: languages_spoken - a list of languages used in reviews for each unique reviewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0845b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = spark.read.csv(dim_model_reviews_new,header=\"True\", inferSchema=\"True\",multiLine=\"True\",escape='\"',ignoreLeadingWhiteSpace=\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38077e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec  = Window.partitionBy(\"reviewer_id\").orderBy(\"date\").rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)                                             \n",
    "df_reviewers = df_reviews \\\n",
    "               .withColumn(\"languages_spoken\", F.collect_set('comment_language').over(windowSpec))\\\n",
    "               .withColumn(\"latest\", F.last(\"date\").over(windowSpec))\\\n",
    "               .filter(\"date == latest\")\\\n",
    "               .dropDuplicates([\"reviewer_id\"])\\\n",
    "               .select(\"reviewer_id\",\"reviewer_name\", \"languages_spoken\", \"date\")\\\n",
    "               .withColumnRenamed(\"date\",\"last_updated\")\n",
    "df_reviewers = df_reviewers.withColumn(\"languages_spoken\",F.array_join(\"languages_spoken\",\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f55259",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviewers.write.csv(dim_model_reviewers_new, escape='\"', header=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9e3a32",
   "metadata": {},
   "source": [
    "### Weather table\n",
    "- uniquely identified by weather_id\n",
    "- shows temperature and rain per date and city\n",
    "- source data: weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6b2f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = spark.read.parquet(path_out_city_temperature_data)\n",
    "df_rain = spark.read.parquet(path_out_city_rain_data)\n",
    "df_stations = spark.read.parquet(path_out_weather_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b333ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.createOrReplaceTempView(\"temp\")\n",
    "df_rain.createOrReplaceTempView(\"rain\")\n",
    "df_stations.createOrReplaceTempView(\"stations\")\n",
    "\n",
    "query=\"\"\"\n",
    "SELECT null as weather_id,to_date(temp.DATE, \"yyyyMMdd\") as date, temp.TG/10 as temperature, rain.RR/10 as rain, stations.city\n",
    "FROM temp\n",
    "JOIN rain\n",
    "ON temp.DATE == rain.DATE\n",
    "AND temp.STAID == rain.STAID\n",
    "JOIN stations\n",
    "ON temp.STAID == stations.STAID\n",
    "WHERE to_date(temp.DATE, \"yyyyMMdd\") > to_date('20090101',\"yyyyMMdd\")\n",
    "ORDER BY date\n",
    "\"\"\"\n",
    "df_weather = spark.sql(query)\n",
    "df_weather = df_weather.withColumn(\"weather_id\",F.concat_ws(\"_\",\"city\", \"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81df61c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather.write.csv(dim_model_weather_new, escape='\"', header=\"true\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
